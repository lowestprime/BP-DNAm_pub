{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954f4982",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# os.environ['CUDA_HOME'] = '/u/local/cuda/12.3'\n",
    "# os.environ['PATH'] = f\"/u/local/cuda/12.3/bin:{os.environ['PATH']}\"\n",
    "from IPython.display import display, HTML\n",
    "# adjust width % as desired\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datetime import datetime\n",
    "# # import pydot\n",
    "# # import graphviz\n",
    "# # import pipdeptree\n",
    "\n",
    "# # !pipdeptree\n",
    "# # !pip freeze > requirements.txt\n",
    "# # Step 1: Create the requirements directory and filenames\n",
    "# # os.makedirs(\"requirements\", exist_ok=True)\n",
    "# current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# filename_basic = f\"requirements_{current_time}.txt\"\n",
    "# filename_detailed = f\"requirements-detailed_{current_time}.txt\"\n",
    "# filename_svg = f\"dependencies_{current_time}.svg\"\n",
    "# filepath_basic = os.path.join(\"requirements\", filename_basic)\n",
    "# filepath_detailed = os.path.join(\"requirements\", filename_detailed)\n",
    "# filepath_svg = os.path.join(\"requirements\", filename_svg)\n",
    "\n",
    "# # Step 2: Run the commands and save the output to files without printing to the cell\n",
    "# !pipdeptree 2>/dev/null | grep -E '^\\w+' > {filepath_basic} 2>/dev/null\n",
    "# !pipdeptree --freeze > {filepath_detailed} 2> /dev/null\n",
    "# !pipdeptree --graph-output dot 2> /dev/null | dot -Tsvg -o {filepath_svg} 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ac963a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import shutil\n",
    "import json\n",
    "import cupy as cp\n",
    "#import cudf\n",
    "#import dask_cudf\n",
    "#from dask_cuda import LocalCUDACluster\n",
    "#import dask\n",
    "#from dask.distributed import Client, wait\n",
    "import pyarrow.feather as fth\n",
    "#import dask.dataframe as dd\n",
    "#import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyaging as pya\n",
    "from pygam import LinearGAM, LogisticGAM, s\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats, sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, prange\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mepylome import Manifest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable experimental features first\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from patsy import dmatrix\n",
    "#import tensorflow as tf\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "#from tensorflow.keras.layers import Input, Dense\n",
    "#from tensorflow.keras.models import Model\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8827c32e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = pya.models.GrimAge2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cfcc93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.metadata.update({\n",
    "    \"clock_name\": 'grimage2',\n",
    "    \"data_type\": 'methylation',\n",
    "    \"species\": 'Homo sapiens',\n",
    "    \"year\": 2022,\n",
    "    \"approved_by_author\": 'âŒ›',\n",
    "    \"citation\": \"Lu, Ake T., et al. \\\"DNA methylation GrimAge version 2.\\\" Aging (Albany NY) 14.23 (2022): 9484.\",\n",
    "    \"doi\": \"https://doi.org/10.18632/aging.204434\",\n",
    "    \"research_only\": True,\n",
    "    \"notes\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f80c7f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----------> Data found in ./grimage2_subcomponents.csv\n",
      "|-----------> Data found in ./grimage2.csv\n",
      "|-----------> Data found in ./datMiniAnnotation3_Gold.csv\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/grimage2_subcomponents.csv\",\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/grimage2.csv\",\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/datMiniAnnotation3_Gold.csv\",\n",
    "]\n",
    "dir = \".\"\n",
    "logger = pya.logger.Logger()\n",
    "\n",
    "for url in urls:\n",
    "    pya.utils.download(url, dir, logger, indent_level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7923e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load feature sets from CSV files\n",
    "df = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "df_grimage = pd.read_csv('grimage2.csv', index_col=0)\n",
    "\n",
    "# Identify features\n",
    "all_features = np.unique(df['var'])[2:].tolist() + ['Female', 'Age']\n",
    "model.features = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076c205",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to load weights for subcomponents\n",
    "def load_model_weights(y_pred, model_attr):\n",
    "    indices = torch.tensor([all_features.index(item) for item in df.loc[df['Y.pred'] == y_pred, 'var'] if item in all_features]).long()\n",
    "    model_layer = pya.models.LinearModel(input_dim=len(indices))\n",
    "    model_layer.linear.weight.data = torch.tensor(df.loc[df['Y.pred'] == y_pred, 'beta'][1:].values).unsqueeze(0).float()\n",
    "    model_layer.linear.bias.data = torch.tensor(df.loc[df['Y.pred'] == y_pred, 'beta'].iloc[0]).float()\n",
    "    setattr(model, model_attr, model_layer)\n",
    "    setattr(model, f'features_{model_attr}', indices)\n",
    "\n",
    "# Apply the function to each subcomponent\n",
    "components = {\n",
    "    'DNAmPACKYRS': 'PACKYRS', \n",
    "    'DNAmadm': 'ADM', \n",
    "    'DNAmB2M': 'B2M',\n",
    "    'DNAmCystatin_C': 'CystatinC', \n",
    "    'DNAmGDF_15': 'GDF15',\n",
    "    'DNAmleptin': 'Leptin',\n",
    "    'DNAmpai_1': 'PAI1',\n",
    "    'DNAmTIMP_1': 'TIMP1',\n",
    "    'DNAmlog.CRP': 'LogCRP',\n",
    "    'DNAmlog.A1C': 'A1C'\n",
    "}\n",
    "\n",
    "for y_pred, model_attr in components.items():\n",
    "    load_model_weights(y_pred, model_attr)\n",
    "\n",
    "# Load base model weights\n",
    "base_model = pya.models.LinearModel(input_dim=len(df_grimage))\n",
    "base_model.linear.weight.data = torch.tensor(df_grimage['beta'].tolist()).unsqueeze(0).float()\n",
    "base_model.linear.bias.data = torch.tensor([0]).float()\n",
    "model.base_model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b599491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('datMiniAnnotation3_Gold.csv', index_col=0)\n",
    "model.reference_values = reference_df.loc[model.features[:-2]]['gold'].tolist() + [1, 65]  # Example: 65-year-old female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e5750",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "weights_dir = \"../weights\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, os.path.join(weights_dir, f\"{model.metadata['clock_name']}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a46ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(methylation_data_subset_pd.columns)\n",
    "missing_features = [feature for feature in model.features if feature not in methylation_data_subset_pd.columns]\n",
    "# print(\"Missing features:\", missing_features)\n",
    "missing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934afd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract relevant features\n",
    "# Adjust the model's features list to include only those present in the data\n",
    "available_features = [feature for feature in model.features if feature in methylation_data_subset_pd.columns]\n",
    "input_data_subset = methylation_data_subset_pd[available_features].values\n",
    "\n",
    "# # Convert to tensor and run the model\n",
    "# input_tensor_final = torch.tensor(input_data_final, dtype=torch.float32)\n",
    "# model.eval()\n",
    "# model.to(float)\n",
    "# pred = model(input_tensor_final)\n",
    "# print(pred)\n",
    "\n",
    "# Convert to tensor and run the model\n",
    "input_tensor_subset = torch.tensor(input_data_subset, dtype=torch.float32)\n",
    "model.eval()\n",
    "model.to(float)\n",
    "pred = model(input_tensor_subset)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a98497",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_folder(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Deleted folder: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {path}: {e}\")\n",
    "\n",
    "# Get a list of all files and folders in the current directory\n",
    "all_items = os.listdir('.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1864b1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for item in os.listdir('.'):\n",
    "    if os.path.isfile(item) and not item.endswith('.ipynb'):\n",
    "        os.remove(item)\n",
    "    elif os.path.isdir(item):\n",
    "        shutil.rmtree(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f094f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
