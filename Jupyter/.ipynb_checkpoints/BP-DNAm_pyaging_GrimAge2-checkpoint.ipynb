{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# os.environ['CUDA_HOME'] = '/u/local/cuda/12.3'\n",
    "# os.environ['PATH'] = f\"/u/local/cuda/12.3/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a100fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip --version\n",
    "# !conda --version\n",
    "# !nvcc --version\n",
    "# !mamba --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env list\n",
    "# !conda list\n",
    "# !mamba env list\n",
    "# !mamba list\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -y -n my-conda-env\n",
    "# !source activate my-conda-env\n",
    "# !conda create -n rapids-24.10 -c rapidsai-nightly -c conda-forge -c nvidia  \\\n",
    "#     rapids=24.10 python=3.9 'cuda-version>=12.0,<=12.5' -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf41b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install cuDF and dask_cudf from the RAPIDS channel\n",
    "# !mamba create --prefix ~/.conda/envs/my_rapids_env -c rapidsai-nightly -c conda-forge -c nvidia  \\\n",
    "#     rapids=24.10 python=3.10 'cuda-version>=12.0,<=12.5' -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install methylsuite\n",
    "# !pip install mepylome\n",
    "# !pip install \\\n",
    "#     --extra-index-url=https://pypi.anaconda.org/rapidsai-wheels-nightly/simple \\\n",
    "#     cudf-cu12 dask-cudf-cu12 dask-cuda --only-binary=:all:\n",
    "# !pip install pandas==2.2.1 FuzzyTM numpy pyaging scipy seaborn matplotlib -U\n",
    "# !pip install numpy pyaging --no-cache-dir -U\n",
    "# !pip cache purge\n",
    "# run in terminal: conda clean --all -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "20ac963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import psutil\n",
    "import inspect\n",
    "import shutil\n",
    "import json\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyaging as pya\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, prange\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mepylome import Manifest\n",
    "import mpmath\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "23ada2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40855c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=540801236992, available=405194600448, percent=25.1, used=106551177216, free=138816040960, active=216581255168, inactive=176222552064, buffers=218198016, cached=295215820800, shared=27965513728, slab=2718806016)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean unused memory\n",
    "gc.collect()\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1f62ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of probes in EPICv2: 930658\n",
      "First few probes: ['cg25497136', 'cg03112325', 'cg03112355', 'cg03112358', 'cg03112360']\n",
      "Number of probes in EPIC: 865918\n",
      "First few probes: ['cg14817997', 'cg26928153', 'cg16269199', 'cg13869341', 'cg14008030']\n",
      "Number of probes in 450k: 485577\n",
      "First few probes: ['cg13869341', 'cg14008030', 'cg12045430', 'cg20826792', 'cg00381604']\n",
      "Number of unique probes in GrimAge2: 1030\n",
      "First few GrimAge2 probes: ['cg00036119', 'cg00102512', 'cg00126959', 'cg00161556', 'cg00252095']\n"
     ]
    }
   ],
   "source": [
    "# Get EPICv2 manifest\n",
    "manifest_450k = Manifest(\"450k\")\n",
    "manifest_epic = Manifest(\"epic\")\n",
    "manifest_epic_v2 = Manifest(\"epicv2\")\n",
    "\n",
    "probes_df_e2 = manifest_epic_v2.data_frame\n",
    "probes_df_e = manifest_epic.data_frame\n",
    "probes_df_450 = manifest_450k.data_frame\n",
    "\n",
    "# Extract probe IDs (assuming 'IlmnID' is the column with probe names)\n",
    "epicv2_probes = probes_df_e2['IlmnID'].tolist()\n",
    "epic_probes = probes_df_e['IlmnID'].tolist()\n",
    "i450_probes = probes_df_450['IlmnID'].tolist()\n",
    "\n",
    "print(f\"Number of probes in EPICv2: {len(epicv2_probes)}\")\n",
    "print(f\"First few probes: {epicv2_probes[:5]}\")\n",
    "print(f\"Number of probes in EPIC: {len(epic_probes)}\")\n",
    "print(f\"First few probes: {epic_probes[:5]}\")\n",
    "print(f\"Number of probes in 450k: {len(i450_probes)}\")\n",
    "print(f\"First few probes: {i450_probes[:5]}\")\n",
    "\n",
    "# Load GrimAge2 feature sets\n",
    "df = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "df_grimage = pd.read_csv('grimage2.csv', index_col=0)\n",
    "\n",
    "# Extract unique probes from GrimAge2\n",
    "grimage2_probes = np.unique(df['var'])[2:].tolist()\n",
    "\n",
    "print(f\"Number of unique probes in GrimAge2: {len(grimage2_probes)}\")\n",
    "print(f\"First few GrimAge2 probes: {grimage2_probes[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "96622337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing GrimAge2 probes in EPICv2: 185\n",
      "First few missing GrimAge2 probes in EPICv2: ['cg00554421', 'cg00564555', 'cg00684178', 'cg00706683', 'cg00892703']\n",
      "Number of missing GrimAge2 probes in EPIC: 1\n",
      "First few missing GrimAge2 probes in EPIC: ['cg24506130']\n",
      "All GrimAge2 probes are present in the i450 set.\n"
     ]
    }
   ],
   "source": [
    "# Check if all GrimAge2 probes are in all arrays\n",
    "missing_probes_epicv2 = [probe for probe in grimage2_probes if probe not in epicv2_probes]\n",
    "missing_probes_epic = [probe for probe in grimage2_probes if probe not in epic_probes]\n",
    "missing_probes_i450 = [probe for probe in grimage2_probes if probe not in i450_probes]\n",
    "\n",
    "if not missing_probes_epicv2:\n",
    "    print(\"All GrimAge2 probes are present in the EPICv2 set.\")\n",
    "else:\n",
    "    print(f\"Number of missing GrimAge2 probes in EPICv2: {len(missing_probes_epicv2)}\")\n",
    "    print(f\"First few missing GrimAge2 probes in EPICv2: {missing_probes_epicv2[:5]}\")\n",
    "if not missing_probes_epic:\n",
    "    print(\"All GrimAge2 probes are present in the EPIC set.\")\n",
    "else:\n",
    "    print(f\"Number of missing GrimAge2 probes in EPIC: {len(missing_probes_epic)}\")\n",
    "    print(f\"First few missing GrimAge2 probes in EPIC: {missing_probes_epic[:5]}\")\n",
    "if not missing_probes_i450:\n",
    "    print(\"All GrimAge2 probes are present in the i450 set.\")\n",
    "else:\n",
    "    print(f\"Number of missing GrimAge2 probes in 450k: {len(missing_probes_i450)}\")\n",
    "    print(f\"First few missing GrimAge2 probes in 450k: {missing_probes_i450[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78fdd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load methylation data subset and final\n",
    "subset_path_parquet = '/u/project/ophoff/cobeaman/Tools/DNAmGrimAgeGitHub/input/mymetharray_subset_2458_r_654_c_08152024_192025.parquet'\n",
    "final_path_parquet = '/u/scratch/c/cobeaman/mymetharray_final_2458_r_731791_c_08152024_192025.parquet'\n",
    "\n",
    "methylation_data_subset = pq.read_table(subset_path_parquet)\n",
    "methylation_data_final = pq.read_table(final_path_parquet)\n",
    "\n",
    "# Convert the cuDF DataFrame to a pandas DataFrame\n",
    "methylation_data_subset_pd = methylation_data_subset.to_pandas()\n",
    "methylation_data_subset_pd.set_index('SampleID', inplace=True)\n",
    "methylation_data_final_pd = methylation_data_final.to_pandas()\n",
    "methylation_data_final_pd.set_index('SampleID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methylation_data_final_pd.to_pickle('/u/scratch/c/cobeaman/mymetharray_final_2458_r_731791_c_08122024_235109.pkl')\n",
    "# methylation_data_subset_pd.to_pickle('/u/project/ophoff/cobeaman/Tools/DNAmGrimAgeGitHub/input/mymetharray_subset_2458_r_654_c_08112024_142045.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "# print(methylation_data_subset['Female'].dtype)\n",
    "\n",
    "# Ensure 'Female' column is binary (0 or 1)\n",
    "# methylation_data_final['Female'] = (methylation_data_final['Female'] == 1).astype(int)\n",
    "# methylation_data_subset['Female'] = (methylation_data_subset['Female'] == 1).astype(int)\n",
    "\n",
    "# Handle any missing data (if necessary)\n",
    "methylation_data_final_pd.dropna(inplace=True)\n",
    "methylation_data_subset_pd.dropna(inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(methylation_data_final_pd.isnull().sum())\n",
    "print(methylation_data_subset_pd.isnull().sum())\n",
    "\n",
    "# Aggregate any duplicated probes\n",
    "# methylation_data_subset_pd = pya.pp.epicv2_probe_aggregation(methylation_data_subset_pd)\n",
    "# methylation_data_final_pd = pya.pp.epicv2_probe_aggregation(methylation_data_final_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def parallel_to_numeric(arr):\n",
    "    result = np.empty(arr.shape, dtype=np.float64)\n",
    "    for i in prange(arr.shape[0]):\n",
    "        for j in prange(arr.shape[1]):\n",
    "            try:\n",
    "                result[i, j] = float(arr[i, j])\n",
    "            except ValueError:\n",
    "                result[i, j] = np.nan\n",
    "    return result\n",
    "\n",
    "def prepare_data_for_adata(df, metadata_cols, chunk_size=1000):\n",
    "    # Separate metadata and data\n",
    "    metadata = df[metadata_cols]\n",
    "    data = df.drop(columns=metadata_cols)\n",
    "    \n",
    "    # First, try to convert all columns to numeric\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Process data in chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[i:i+chunk_size]\n",
    "        \n",
    "        # Convert to GPU array\n",
    "        chunk_gpu = cp.array(chunk.values, dtype=cp.float64)\n",
    "        \n",
    "        # Transfer back to CPU\n",
    "        chunk_np = cp.asnumpy(chunk_gpu)\n",
    "        \n",
    "        chunks.append(pd.DataFrame(chunk_np, columns=chunk.columns, index=chunk.index))\n",
    "    \n",
    "    # Combine processed chunks\n",
    "    data_processed = pd.concat(chunks)\n",
    "    \n",
    "    # Combine metadata and converted data\n",
    "    df_prepared = pd.concat([metadata, data_processed], axis=1)\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "# Prepare the data\n",
    "methylation_data_final_pd_prepared = prepare_data_for_adata(methylation_data_final_pd, ['Female', 'Age', 'Diagnosis'])\n",
    "methylation_data_subset_pd_prepared = prepare_data_for_adata(methylation_data_subset_pd, ['Female', 'Age', 'Diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [methylation_data_final_pd_prepared, methylation_data_subset_pd_prepared]:\n",
    "    df['female'] = df['Female']\n",
    "    df['age'] = df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GrimAge2 feature sets\n",
    "df = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "df_grimage = pd.read_csv('grimage2.csv', index_col=0)\n",
    "\n",
    "# Extract unique probes from GrimAge2\n",
    "grimage2_probes = np.unique(df['var'])[2:].tolist()\n",
    "\n",
    "print(f\"Number of unique probes in GrimAge2: {len(grimage2_probes)}\")\n",
    "print(f\"First few GrimAge2 probes: {grimage2_probes[:5]}\")\n",
    "\n",
    "# identify missing GrimAge2 probes in methylation data\n",
    "grimage2_missing_features = [probe for probe in grimage2_probes if probe not in methylation_data_final_pd_prepared.columns]\n",
    "\n",
    "# print(\"Missing features:\", missing_features)\n",
    "if not grimage2_missing_features:\n",
    "    print(\"All GrimAge2 probes are present in the methylation data.\")\n",
    "else:\n",
    "    print(f\"Number of missing GrimAge2 probes in methylation data: {len(grimage2_missing_features)}\")\n",
    "    print(f\"First few missing GrimAge2 probes in methylation data: {grimage2_missing_features[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation and Exploratory Analysis\n",
    "def prepare_data(df, missing_features):\n",
    "    # Separate methylation data from metadata\n",
    "    methylation_data = df[[col for col in df.columns if col.startswith('cg')]]\n",
    "    metadata = df[['female', 'age', 'Diagnosis']]\n",
    "    \n",
    "    # Add missing columns\n",
    "    for col in missing_features:\n",
    "        if col not in methylation_data.columns and col.startswith('cg'):\n",
    "            methylation_data[col] = np.nan\n",
    "    \n",
    "    return methylation_data, metadata\n",
    "\n",
    "def exploratory_analysis(df):\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "    print(f\"Average percentage of missing values: {missing_percentages.mean():.2f}%\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(missing_percentages, bins=50)\n",
    "    plt.title(\"Distribution of Missing Values Percentage\")\n",
    "    plt.xlabel(\"Percentage of Missing Values\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Initial Imputation with KNN\n",
    "def knn_impute(df, n_neighbors=5):\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Advanced Imputation with MICE and Random Forest\n",
    "def mice_rf_impute(df, max_iter=10, n_estimators=100, random_state=0):\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=n_estimators, random_state=random_state),\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# 4. Deep Learning Imputation\n",
    "def create_autoencoder(input_dim, encoding_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "def deep_learning_impute(df, epochs=100, batch_size=32):\n",
    "    # Normalize data to [0, 1] range (suitable for beta values)\n",
    "    normalized_data = df.values\n",
    "    \n",
    "    # Create a mask for missing values\n",
    "    missing_mask = np.isnan(normalized_data)\n",
    "    \n",
    "    # Replace NaNs with mean for initial input\n",
    "    col_mean = np.nanmean(normalized_data, axis=0)\n",
    "    normalized_data[missing_mask] = np.take(col_mean, missing_mask.nonzero()[1])\n",
    "    \n",
    "    # Create and train the autoencoder\n",
    "    input_dim = df.shape[1]\n",
    "    encoding_dim = min(input_dim // 2, 256)  # Cap encoding dim to prevent overfitting\n",
    "    autoencoder = create_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    autoencoder.fit(normalized_data, normalized_data, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=0)\n",
    "    \n",
    "    # Use the model to impute missing values\n",
    "    imputed_data = autoencoder.predict(normalized_data)\n",
    "    \n",
    "    # Replace only the missing values in the original data\n",
    "    normalized_data[missing_mask] = imputed_data[missing_mask]\n",
    "    \n",
    "    return pd.DataFrame(normalized_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# 5. Ensemble Method\n",
    "def ensemble_impute(df, methods=['knn', 'mice_rf', 'deep_learning']):\n",
    "    imputed_dfs = []\n",
    "    \n",
    "    if 'knn' in methods:\n",
    "        imputed_dfs.append(knn_impute(df))\n",
    "    if 'mice_rf' in methods:\n",
    "        imputed_dfs.append(mice_rf_impute(df))\n",
    "    if 'deep_learning' in methods:\n",
    "        imputed_dfs.append(deep_learning_impute(df))\n",
    "    \n",
    "    # Average the results from different methods\n",
    "    ensemble_imputed = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "    return ensemble_imputed\n",
    "\n",
    "# 6. Validation and Sensitivity Analysis\n",
    "def validate_imputation(original_df, imputed_df, n_samples=1000):\n",
    "    # Select a subset of non-missing values to compare\n",
    "    non_missing_mask = ~original_df.isnull().any(axis=1)\n",
    "    sample_indices = np.random.choice(non_missing_mask.index[non_missing_mask], size=n_samples, replace=False)\n",
    "    \n",
    "    original_sample = original_df.loc[sample_indices]\n",
    "    imputed_sample = imputed_df.loc[sample_indices]\n",
    "    \n",
    "    mse = mean_squared_error(original_sample.values.flatten(), imputed_sample.values.flatten())\n",
    "    correlation, _ = pearsonr(original_sample.values.flatten(), imputed_sample.values.flatten())\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Pearson Correlation: {correlation:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(original_sample.values.flatten(), imputed_sample.values.flatten(), alpha=0.1)\n",
    "    plt.xlabel(\"Original Values\")\n",
    "    plt.ylabel(\"Imputed Values\")\n",
    "    plt.title(\"Original vs Imputed Values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main workflow\n",
    "def imputation_workflow(df, missing_features):\n",
    "    methylation_data, metadata = prepare_data(df, missing_features)\n",
    "    \n",
    "    print(\"Exploratory Analysis:\")\n",
    "    exploratory_analysis(methylation_data)\n",
    "    \n",
    "    print(\"\\nPerforming KNN Imputation...\")\n",
    "    knn_imputed = knn_impute(methylation_data)\n",
    "    \n",
    "    print(\"\\nPerforming MICE with Random Forest Imputation...\")\n",
    "    mice_rf_imputed = mice_rf_impute(methylation_data)\n",
    "    \n",
    "    print(\"\\nPerforming Deep Learning Imputation...\")\n",
    "    dl_imputed = deep_learning_impute(methylation_data)\n",
    "    \n",
    "    print(\"\\nPerforming Ensemble Imputation...\")\n",
    "    ensemble_imputed = ensemble_impute(methylation_data)\n",
    "    \n",
    "    print(\"\\nValidation and Sensitivity Analysis:\")\n",
    "    validate_imputation(methylation_data, ensemble_imputed)\n",
    "    \n",
    "    # Combine imputed data with metadata\n",
    "    final_imputed_data = pd.concat([ensemble_imputed, metadata], axis=1)\n",
    "    \n",
    "    return final_imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0965d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputed_methylation_data = imputation_workflow(methylation_data_final_pd_prepared, grimage2_missing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d232632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to AnnData objects\n",
    "adata_final = pya.pp.df_to_adata(methylation_data_final_pd_prepared, metadata_cols=['Female','Age','Diagnosis'], imputer_strategy='knn')\n",
    "adata_subset = pya.pp.df_to_adata(methylation_data_subset_pd_prepared, metadata_cols=['Female','Age','Diagnosis'], imputer_strategy='knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91949e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_subset.uns\n",
    "methylation_data_subset_pd_prepared.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab805e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict age using GrimAge2 for both datasets\n",
    "pya.pred.predict_age(adata_final, ['GrimAge2'])\n",
    "pya.pred.predict_age(adata_subset, ['GrimAge2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the columns in the obs DataFrame\n",
    "print(adata_subset.obs.columns)\n",
    "print(adata_final.obs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ad3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows of predictions\n",
    "# print(adata_final.obs[['GrimAge2']].head())\n",
    "print(adata_subset.obs.head())\n",
    "print(adata_final.obs.head())\n",
    "\n",
    "# Save predictions to CSV files\n",
    "# adata_final.obs[['GrimAge2']].to_csv('GrimAge2_predictions_final.csv')\n",
    "adata_subset.obs.to_csv('GrimAge2_predictions_subset.csv')\n",
    "adata_final.obs.to_csv('GrimAge2_predictions_subset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ed9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf with NaN and drop rows where either value is NaN\n",
    "valid_data = adata_subset.obs[['Age', 'grimage2']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Extract the synchronized age and grimage2 arrays\n",
    "age_clean = valid_data['Age']\n",
    "grimage2_clean = valid_data['grimage2']\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr, pearson_p_value = stats.pearsonr(age_clean, grimage2_clean)\n",
    "\n",
    "# Calculate Spearman correlation (robust to non-linearity)\n",
    "spearman_corr, spearman_p_value = stats.spearmanr(age_clean, grimage2_clean)\n",
    "\n",
    "# Calculate Kendall correlation (robust to outliers)\n",
    "kendall_corr, kendall_p_value = stats.kendalltau(age_clean, grimage2_clean)\n",
    "\n",
    "# Display the correlation results\n",
    "# Define a threshold for extremely small p-values\n",
    "threshold = 1e-320\n",
    "\n",
    "# Display the correlation results with adjusted p-values\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f}, p-value: {'< {:.0e}'.format(threshold) if pearson_p_value < threshold else '{:.3e}'.format(pearson_p_value)}\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f}, p-value: {'< {:.0e}'.format(threshold) if spearman_p_value < threshold else '{:.3e}'.format(spearman_p_value)}\")\n",
    "print(f\"Kendall correlation: {kendall_corr:.3f}, p-value: {'< {:.0e}'.format(threshold) if kendall_p_value < threshold else '{:.3e}'.format(kendall_p_value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the theme and style for the plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure and axis objects\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Define a modern color palette with strong contrast and color-blind safety\n",
    "colors = ['#0077BB', '#EE3377', '#EE7733']  # Vivid Blue, Vibrant Pink, Bright Orange\n",
    "\n",
    "# Use seaborn's regplot for scatter plot with regression line\n",
    "sns.regplot(\n",
    "    x=age_clean,\n",
    "    y=grimage2_clean,\n",
    "    ci=95,\n",
    "    scatter_kws={\"s\": 40, \"alpha\": 0.2, \"color\": colors[0]},  # Vivid Blue points with adjusted opacity\n",
    "    line_kws={\"color\": colors[1], \"linewidth\": 3, \"alpha\": 1},  # Vibrant Pink regression line, thicker and more opaque\n",
    ")\n",
    "\n",
    "# Add a line plot with confidence interval\n",
    "sns.lineplot(\n",
    "    x=age_clean,\n",
    "    y=grimage2_clean,\n",
    "    errorbar=('ci', 99),\n",
    "    lw=2.5,  # Increase line width for better visibility\n",
    "    color=colors[2],  # Bright Orange line plot\n",
    "    alpha=0.5,  # Increase opacity for confidence intervals\n",
    ")\n",
    "\n",
    "# Add titles and labels with enhanced font size\n",
    "plt.title('Correlation between Age and GrimAge2', fontsize=24, weight='bold', pad=20)\n",
    "plt.xlabel('Age (Yrs)', fontsize=18, weight='bold')\n",
    "plt.ylabel('GrimAge2 (Yrs)', fontsize=18, weight='bold')\n",
    "\n",
    "# Define a threshold for extremely small p-values\n",
    "threshold = 1e-320\n",
    "\n",
    "# Create a legend with correlation statistics\n",
    "legend_text = (\n",
    "    f\"Pearson r = {pearson_corr:.3f}, p = {'< {:.0e}'.format(threshold) if pearson_p_value < threshold else '{:.3e}'.format(pearson_p_value)}\\n\"\n",
    "    f\"Spearman ρ = {spearman_corr:.3f}, p = {'< {:.0e}'.format(threshold) if spearman_p_value < threshold else '{:.3e}'.format(spearman_p_value)}\\n\"\n",
    "    f\"Kendall τ = {kendall_corr:.3f}, p = {'< {:.0e}'.format(threshold) if kendall_p_value < threshold else '{:.3e}'.format(kendall_p_value)}\"\n",
    ")\n",
    "plt.text(0.05, 0.95, legend_text, ha='left', va='top', transform=plt.gca().transAxes, fontsize=16,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.4\", edgecolor=\"gray\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Customize the ticks and labels for better readability\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(visible=True, color='gray', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Remove the top and right spines for a cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Adjust the plot margins\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an SVG file for high resolution\n",
    "plt.savefig('Age_vs_GrimAge2_Correlation_Accessible.svg', format='svg', dpi=4000, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ScatterChart, Scatter, XAxis, YAxis, ZAxis, Tooltip, Legend, ResponsiveContainer } from 'recharts';\n",
    "\n",
    "const generateMockData = (n) => {\n",
    "  const data = [];\n",
    "  for (let i = 0; i < n; i++) {\n",
    "    const age = Math.floor(Math.random() * 60) + 20;\n",
    "    const grimAge = age + (Math.random() * 20 - 10);\n",
    "    const isBipolar = Math.random() > 0.5;\n",
    "    data.push({\n",
    "      age: age,\n",
    "      grimAge: grimAge,\n",
    "      group: isBipolar ? 'Bipolar' : 'Control',\n",
    "    });\n",
    "  }\n",
    "  return data;\n",
    "};\n",
    "\n",
    "const data = generateMockData(100);\n",
    "\n",
    "const AgeCorrelationPlot = () => {\n",
    "  return (\n",
    "    <ResponsiveContainer width=\"100%\" height={400}>\n",
    "      <ScatterChart margin={{ top: 20, right: 20, bottom: 20, left: 20 }}>\n",
    "        <XAxis type=\"number\" dataKey=\"age\" name=\"Chronological Age\" unit=\"years\" />\n",
    "        <YAxis type=\"number\" dataKey=\"grimAge\" name=\"Biological Age (GrimAge)\" unit=\"years\" />\n",
    "        <ZAxis type=\"category\" dataKey=\"group\" name=\"Group\" />\n",
    "        <Tooltip cursor={{ strokeDasharray: '3 3' }} />\n",
    "        <Legend />\n",
    "        <Scatter name=\"Bipolar\" data={data.filter(d => d.group === 'Bipolar')} fill=\"#8884d8\" />\n",
    "        <Scatter name=\"Control\" data={data.filter(d => d.group === 'Control')} fill=\"#82ca9d\" />\n",
    "      </ScatterChart>\n",
    "    </ResponsiveContainer>\n",
    "  );\n",
    "};\n",
    "\n",
    "export default AgeCorrelationPlot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ace1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming adata_subset.obs is your DataFrame\n",
    "df = adata_subset.obs.copy()\n",
    "\n",
    "# Replace inf with NaN and drop rows where either value is NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(subset=['Age', 'grimage2'], inplace=True)\n",
    "\n",
    "# Calculate the difference between GrimAge2 and Age\n",
    "df['Difference'] = df['grimage2'] - df['Age']\n",
    "\n",
    "# Standardize the difference for clustering\n",
    "scaler = StandardScaler()\n",
    "df['Scaled_Difference'] = scaler.fit_transform(df[['Difference']])\n",
    "\n",
    "# Perform K-means clustering, experimenting with different cluster numbers\n",
    "n_clusters = 2  # Two clusters: Aligned and Accelerated Aging\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(df[['Scaled_Difference']])\n",
    "\n",
    "# Map the clusters to meaningful names\n",
    "df['Group'] = df['Cluster'].replace({0: 'Aligned', 1: 'Accelerated Aging'})\n",
    "\n",
    "# Replace 'Female' column values with 'Female' and 'Male' labels\n",
    "df['Sex'] = df['Female'].replace({1.0: 'Female', 0.0: 'Male'})\n",
    "\n",
    "# Create a new column that combines Group and Sex for coloring\n",
    "df['Group_Sex'] = df['Group'] + ' - ' + df['Sex']\n",
    "\n",
    "# Use a colorblind-safe palette with higher contrast\n",
    "color_palette = {\n",
    "    'Aligned - Female': '#0072B2',       # Bright blue\n",
    "    'Aligned - Male': '#D55E00',         # Bright orange\n",
    "    'Accelerated Aging - Female': '#009E73',  # Bright green\n",
    "    'Accelerated Aging - Male': '#F0E442'     # Bright yellow\n",
    "}\n",
    "\n",
    "# Plot the Groups and indicate sex\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "scatter = sns.scatterplot(\n",
    "    data=df,\n",
    "    x='Age',\n",
    "    y='grimage2',\n",
    "    hue='Group_Sex',\n",
    "    style='Group_Sex',  # Indicating sex by marker style\n",
    "    markers={'Aligned - Female': 'o', 'Aligned - Male': 's', 'Accelerated Aging - Female': '^', 'Accelerated Aging - Male': 'X'},\n",
    "    palette=color_palette,\n",
    "    s=120,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Define the concept of \"Alignment\" in the title\n",
    "alignment_text = \"Alignment = Grimage2 - Age (Aligned: within ±5 years or Decelerated, Accelerated: > 10 years)\"\n",
    "plt.title(f'Age vs GrimAge2: Clustered by Biological Age Alignment and Indicated by Sex\\n{alignment_text}', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Age (Years)', fontsize=18)\n",
    "plt.ylabel('GrimAge2 (Years)', fontsize=18)\n",
    "plt.legend(title='Group & Sex', fontsize=12, title_fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('Age_vs_GrimAge2_Biological_Age_Alignment_Sex_Clustering_Optimized.svg', format='svg', dpi=4000, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary of group characteristics\n",
    "group_summary = df.groupby(['Group', 'Sex']).agg({\n",
    "    'Age': ['mean', 'std'],\n",
    "    'grimage2': ['mean', 'std'],\n",
    "    'Difference': ['mean', 'std'],\n",
    "    'Group': 'size'\n",
    "}).rename(columns={'Group': 'Count'}).reset_index()\n",
    "\n",
    "print(group_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pya.models.GrimAge2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metadata.update({\n",
    "    \"clock_name\": 'grimage2',\n",
    "    \"data_type\": 'methylation',\n",
    "    \"species\": 'Homo sapiens',\n",
    "    \"year\": 2022,\n",
    "    \"approved_by_author\": '⌛',\n",
    "    \"citation\": \"Lu, Ake T., et al. \\\"DNA methylation GrimAge version 2.\\\" Aging (Albany NY) 14.23 (2022): 9484.\",\n",
    "    \"doi\": \"https://doi.org/10.18632/aging.204434\",\n",
    "    \"research_only\": True,\n",
    "    \"notes\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/grimage2_subcomponents.csv\",\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/grimage2.csv\",\n",
    "    \"https://pyaging.s3.amazonaws.com/supporting_files/datMiniAnnotation3_Gold.csv\",\n",
    "]\n",
    "dir = \".\"\n",
    "logger = pya.logger.Logger()\n",
    "\n",
    "for url in urls:\n",
    "    pya.utils.download(url, dir, logger, indent_level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature sets from CSV files\n",
    "df = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "df_grimage = pd.read_csv('grimage2.csv', index_col=0)\n",
    "\n",
    "# Identify features\n",
    "all_features = np.unique(df['var'])[2:].tolist() + ['Female', 'Age']\n",
    "model.features = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load weights for subcomponents\n",
    "def load_model_weights(y_pred, model_attr):\n",
    "    indices = torch.tensor([all_features.index(item) for item in df.loc[df['Y.pred'] == y_pred, 'var'] if item in all_features]).long()\n",
    "    model_layer = pya.models.LinearModel(input_dim=len(indices))\n",
    "    model_layer.linear.weight.data = torch.tensor(df.loc[df['Y.pred'] == y_pred, 'beta'][1:].values).unsqueeze(0).float()\n",
    "    model_layer.linear.bias.data = torch.tensor(df.loc[df['Y.pred'] == y_pred, 'beta'].iloc[0]).float()\n",
    "    setattr(model, model_attr, model_layer)\n",
    "    setattr(model, f'features_{model_attr}', indices)\n",
    "\n",
    "# Apply the function to each subcomponent\n",
    "components = {\n",
    "    'DNAmPACKYRS': 'PACKYRS', \n",
    "    'DNAmadm': 'ADM', \n",
    "    'DNAmB2M': 'B2M',\n",
    "    'DNAmCystatin_C': 'CystatinC', \n",
    "    'DNAmGDF_15': 'GDF15',\n",
    "    'DNAmleptin': 'Leptin',\n",
    "    'DNAmpai_1': 'PAI1',\n",
    "    'DNAmTIMP_1': 'TIMP1',\n",
    "    'DNAmlog.CRP': 'LogCRP',\n",
    "    'DNAmlog.A1C': 'A1C'\n",
    "}\n",
    "\n",
    "for y_pred, model_attr in components.items():\n",
    "    load_model_weights(y_pred, model_attr)\n",
    "\n",
    "# Load base model weights\n",
    "base_model = pya.models.LinearModel(input_dim=len(df_grimage))\n",
    "base_model.linear.weight.data = torch.tensor(df_grimage['beta'].tolist()).unsqueeze(0).float()\n",
    "base_model.linear.bias.data = torch.tensor([0]).float()\n",
    "model.base_model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b599491",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = pd.read_csv('datMiniAnnotation3_Gold.csv', index_col=0)\n",
    "model.reference_values = reference_df.loc[model.features[:-2]]['gold'].tolist() + [1, 65]  # Example: 65-year-old female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.preprocess_name = None\n",
    "model.preprocess_dependencies = None\n",
    "model.postprocess_name = 'cox_to_years'\n",
    "model.postprocess_dependencies = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a567e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pya.utils.print_model_details(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "weights_dir = \"../weights\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, os.path.join(weights_dir, f\"{model.metadata['clock_name']}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(methylation_data_subset_pd.columns)\n",
    "missing_features = [feature for feature in model.features if feature not in methylation_data_subset_pd.columns]\n",
    "# print(\"Missing features:\", missing_features)\n",
    "missing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant features\n",
    "# Adjust the model's features list to include only those present in the data\n",
    "available_features = [feature for feature in model.features if feature in methylation_data_subset_pd.columns]\n",
    "input_data_subset = methylation_data_subset_pd[available_features].values\n",
    "\n",
    "# # Convert to tensor and run the model\n",
    "# input_tensor_final = torch.tensor(input_data_final, dtype=torch.float32)\n",
    "# model.eval()\n",
    "# model.to(float)\n",
    "# pred = model(input_tensor_final)\n",
    "# print(pred)\n",
    "\n",
    "# Convert to tensor and run the model\n",
    "input_tensor_subset = torch.tensor(input_data_subset, dtype=torch.float32)\n",
    "model.eval()\n",
    "model.to(float)\n",
    "pred = model(input_tensor_subset)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a98497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_folder(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Deleted folder: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder {path}: {e}\")\n",
    "\n",
    "# Get a list of all files and folders in the current directory\n",
    "all_items = os.listdir('.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1864b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in os.listdir('.'):\n",
    "    if os.path.isfile(item) and not item.endswith('.ipynb'):\n",
    "        os.remove(item)\n",
    "    elif os.path.isdir(item):\n",
    "        shutil.rmtree(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
