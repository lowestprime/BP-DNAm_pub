{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import shutil\n",
    "import json\n",
    "import cupy as cp\n",
    "#import cudf\n",
    "#import dask_cudf\n",
    "#from dask_cuda import LocalCUDACluster\n",
    "#import dask\n",
    "#from dask.distributed import Client, wait\n",
    "import pyarrow.feather as fth\n",
    "#import dask.dataframe as dd\n",
    "#import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyaging as pya\n",
    "from pygam import LinearGAM, LogisticGAM, s\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats, sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, prange\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mepylome import Manifest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # Enable experimental features first\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from patsy import dmatrix\n",
    "#import tensorflow as tf\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "#from tensorflow.keras.layers import Input, Dense\n",
    "#from tensorflow.keras.models import Model\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad478d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e79575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for Feather files\n",
    "subset_path_feather = '/u/scratch/c/cobeaman/mymetharray_subset_2458_r_654_c_08152024_192025.feather'\n",
    "final_path_feather = '/u/scratch/c/cobeaman/mymetharray_final_2458_r_731791_c_08152024_192025.feather'\n",
    "\n",
    "# Load the data using the single-threaded approach\n",
    "methylation_data_subset = fth.read_table(subset_path_feather)\n",
    "methylation_data_final = fth.read_table(final_path_feather)\n",
    "\n",
    "# Convert to pandas DataFrame and set 'SampleID' as index\n",
    "methylation_data_subset_pd = methylation_data_subset.to_pandas().set_index('SampleID')\n",
    "methylation_data_final_pd = methylation_data_final.to_pandas().set_index('SampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bebb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "# print(methylation_data_subset['Female'].dtype)\n",
    "\n",
    "# Ensure 'Female' column is binary (0 or 1)\n",
    "# methylation_data_final['Female'] = (methylation_data_final['Female'] == 1).astype(int)\n",
    "# methylation_data_subset['Female'] = (methylation_data_subset['Female'] == 1).astype(int)\n",
    "\n",
    "# Handle any missing data (if necessary)\n",
    "methylation_data_final_pd.dropna(inplace=True)\n",
    "methylation_data_subset_pd.dropna(inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(methylation_data_final_pd.isnull().sum())\n",
    "print(methylation_data_subset_pd.isnull().sum())\n",
    "\n",
    "# Aggregate any duplicated probes\n",
    "# methylation_data_subset_pd = pya.pp.epicv2_probe_aggregation(methylation_data_subset_pd)\n",
    "# methylation_data_final_pd = pya.pp.epicv2_probe_aggregation(methylation_data_final_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def parallel_to_numeric(arr):\n",
    "    result = np.empty(arr.shape, dtype=np.float64)\n",
    "    for i in prange(arr.shape[0]):\n",
    "        for j in prange(arr.shape[1]):\n",
    "            try:\n",
    "                result[i, j] = float(arr[i, j])\n",
    "            except ValueError:\n",
    "                result[i, j] = np.nan\n",
    "    return result\n",
    "\n",
    "def prepare_data_for_adata(df, metadata_cols, chunk_size=1000):\n",
    "    # Separate metadata and data\n",
    "    metadata = df[metadata_cols]\n",
    "    data = df.drop(columns=metadata_cols)\n",
    "    \n",
    "    # First, try to convert all columns to numeric\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Process data in chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data.iloc[i:i+chunk_size]\n",
    "        \n",
    "        # Convert to GPU array\n",
    "        chunk_gpu = cp.array(chunk.values, dtype=cp.float64)\n",
    "        \n",
    "        # Transfer back to CPU\n",
    "        chunk_np = cp.asnumpy(chunk_gpu)\n",
    "        \n",
    "        chunks.append(pd.DataFrame(chunk_np, columns=chunk.columns, index=chunk.index))\n",
    "    \n",
    "    # Combine processed chunks\n",
    "    data_processed = pd.concat(chunks)\n",
    "    \n",
    "    # Combine metadata and converted data\n",
    "    df_prepared = pd.concat([metadata, data_processed], axis=1)\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "# Prepare the data\n",
    "methylation_data_final_pd_prepared = prepare_data_for_adata(methylation_data_final_pd, ['Female', 'Age', 'Diagnosis'])\n",
    "methylation_data_subset_pd_prepared = prepare_data_for_adata(methylation_data_subset_pd, ['Female', 'Age', 'Diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [methylation_data_final_pd_prepared, methylation_data_subset_pd_prepared]:\n",
    "    df['female'] = df['Female']\n",
    "    df['age'] = df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6093ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get probe data from manifests\n",
    "def get_probes_from_manifest(manifest_name):\n",
    "    manifest = Manifest(manifest_name)\n",
    "    return set(manifest.data_frame['IlmnID'])\n",
    "\n",
    "# Extract probe IDs from each array as sets for faster lookups\n",
    "epicv2_probes_set = get_probes_from_manifest(\"epicv2\")\n",
    "epic_probes_set = get_probes_from_manifest(\"epic\")\n",
    "i450_probes_set = get_probes_from_manifest(\"450k\")\n",
    "\n",
    "# Load GrimAge2 feature sets and extract unique probes as a set\n",
    "df_grimage_subcomponents = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "grimage2_probes_set = set([probe for probe in df_grimage_subcomponents['var'].unique() if probe.startswith('cg')])\n",
    "\n",
    "# Filter probe names that start with 'cg' from the methylation data\n",
    "cg_probes_set = set([col for col in methylation_data_final_pd_prepared.columns if col.startswith('cg')])\n",
    "\n",
    "# Check for missing probes in each array using set difference for efficiency\n",
    "missing_probes_epicv2 = grimage2_probes_set - epicv2_probes_set\n",
    "missing_probes_epic = grimage2_probes_set - epic_probes_set\n",
    "missing_probes_i450 = grimage2_probes_set - i450_probes_set\n",
    "\n",
    "missing_probes_epicv2_BP = cg_probes_set - epicv2_probes_set\n",
    "missing_probes_epic_BP = cg_probes_set - epic_probes_set\n",
    "missing_probes_i450_BP = cg_probes_set - i450_probes_set\n",
    "\n",
    "# Organize the summary data\n",
    "summary_data = {\n",
    "    'Array': ['EPICv2', 'EPIC', '450k', 'GrimAge2'],\n",
    "    'Total Probes': [\n",
    "        len(epicv2_probes_set),\n",
    "        len(epic_probes_set),\n",
    "        len(i450_probes_set),\n",
    "        len(grimage2_probes_set)\n",
    "    ],\n",
    "    'First Few Probes': [\n",
    "        ', '.join(list(epicv2_probes_set)[:5]),\n",
    "        ', '.join(list(epic_probes_set)[:5]),\n",
    "        ', '.join(list(i450_probes_set)[:5]),\n",
    "        ', '.join(list(grimage2_probes_set)[:5])\n",
    "    ],\n",
    "    'Missing Probes in GrimAge2': [\n",
    "        len(missing_probes_epicv2),\n",
    "        len(missing_probes_epic),\n",
    "        len(missing_probes_i450),\n",
    "        None\n",
    "    ],\n",
    "    'First Few Missing in GrimAge2': [\n",
    "        ', '.join(list(missing_probes_epicv2)[:5]),\n",
    "        ', '.join(list(missing_probes_epic)[:5]),\n",
    "        ', '.join(list(missing_probes_i450)[:5]),\n",
    "        None\n",
    "    ],\n",
    "    'Missing Probes in BPDNAm': [\n",
    "        len(missing_probes_epicv2_BP),\n",
    "        len(missing_probes_epic_BP),\n",
    "        len(missing_probes_i450_BP),\n",
    "        None\n",
    "    ],\n",
    "    'First Few Missing in BPDNAm': [\n",
    "        ', '.join(list(missing_probes_epicv2_BP)[:5]),\n",
    "        ', '.join(list(missing_probes_epic_BP)[:5]),\n",
    "        ', '.join(list(missing_probes_i450_BP)[:5]),\n",
    "        None\n",
    "    ],\n",
    "    'Number of Missing GrimAge2 Probes in Methylation Data': [\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        len(grimage2_probes_set - cg_probes_set)\n",
    "    ],\n",
    "    'First Few Missing GrimAge2 Probes in Methylation Data': [\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        ', '.join(list(grimage2_probes_set - cg_probes_set)[:5])\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for exporting\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV for sharing\n",
    "summary_df.to_csv('probe_summary_detailed.csv', index=False)\n",
    "\n",
    "# Save to Excel for sharing\n",
    "summary_df.to_excel('probe_summary_detailed.xlsx', index=False)\n",
    "\n",
    "# Optional: Display the summary\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to get probe data from manifests\n",
    "def get_probes_from_manifest(manifest_name):\n",
    "    manifest = Manifest(manifest_name)\n",
    "    return set(manifest.data_frame['IlmnID'])\n",
    "\n",
    "# Extract probe IDs from EPICv2 as a set for faster lookups\n",
    "epicv2_probes_set = get_probes_from_manifest(\"epicv2\")\n",
    "\n",
    "# Load GrimAge2 feature sets and extract unique probes as a set\n",
    "df_grimage_subcomponents = pd.read_csv('grimage2_subcomponents.csv', index_col=0)\n",
    "grimage2_probes_set = set([probe for probe in df_grimage_subcomponents['var'].unique() if probe.startswith('cg')])\n",
    "\n",
    "# Filter probe names that start with 'cg' from the methylation data\n",
    "cg_probes_set = set([col for col in methylation_data_final_pd_prepared.columns if col.startswith('cg')])\n",
    "\n",
    "# Calculate specific missing probes\n",
    "missing_grimage2_in_epicv2 = grimage2_probes_set - epicv2_probes_set\n",
    "missing_grimage2_in_methylation = grimage2_probes_set - cg_probes_set\n",
    "\n",
    "# Probes missing in methylation but not in EPICv2\n",
    "missing_in_methylation_only = missing_grimage2_in_methylation - missing_grimage2_in_epicv2\n",
    "\n",
    "# Create a detailed summary DataFrame with the necessary missing probes\n",
    "missing_specific_summary_df = pd.DataFrame({\n",
    "    'Description': [\n",
    "        'GrimAge2 probes missing in EPICv2',\n",
    "        'GrimAge2 probes missing in methylation data',\n",
    "        'GrimAge2 probes missing in only methylation but not EPICv2'\n",
    "    ],\n",
    "    'Number of Missing Probes': [\n",
    "        len(missing_grimage2_in_epicv2),\n",
    "        len(missing_grimage2_in_methylation),\n",
    "        len(missing_in_methylation_only)\n",
    "    ],\n",
    "    'Missing Probes': [\n",
    "        ', '.join(list(missing_grimage2_in_epicv2)),\n",
    "        ', '.join(list(missing_grimage2_in_methylation)),\n",
    "        ', '.join(list(missing_in_methylation_only))\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by 'Number of Missing Probes' in descending order\n",
    "missing_specific_summary_df = missing_specific_summary_df.sort_values(by='Number of Missing Probes', ascending=False)\n",
    "\n",
    "# Save to CSV for sharing\n",
    "missing_specific_summary_df.to_csv('detailed_missing_probes_summary.csv', index=False)\n",
    "\n",
    "# Save to Excel for sharing\n",
    "missing_specific_summary_df.to_excel('detailed_missing_probes_summary.xlsx', index=False)\n",
    "\n",
    "# Optional: Display the summary\n",
    "print(missing_specific_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d681d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Data Preparation and Exploratory Analysis\n",
    "# def prepare_data(df, missing_features):\n",
    "#     # Separate methylation data from metadata\n",
    "#     methylation_data = df[[col for col in df.columns if col.startswith('cg')]]\n",
    "#     metadata = df[['female', 'age', 'Diagnosis']]\n",
    "    \n",
    "#     # Add missing columns\n",
    "#     for col in missing_features:\n",
    "#         if col not in methylation_data.columns and col.startswith('cg'):\n",
    "#             methylation_data[col] = np.nan\n",
    "    \n",
    "#     return methylation_data, metadata\n",
    "\n",
    "# def exploratory_analysis(df):\n",
    "#     missing_percentages = df.isnull().mean() * 100\n",
    "#     print(f\"Average percentage of missing values: {missing_percentages.mean():.2f}%\")\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sns.histplot(missing_percentages, bins=50)\n",
    "#     plt.title(\"Distribution of Missing Values Percentage\")\n",
    "#     plt.xlabel(\"Percentage of Missing Values\")\n",
    "#     plt.show()\n",
    "\n",
    "# # 2. Initial Imputation with KNN\n",
    "# def knn_impute(df, n_neighbors=5):\n",
    "#     imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#     imputed_data = imputer.fit_transform(df)\n",
    "#     return pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# # 3. Advanced Imputation with MICE and Random Forest\n",
    "# def mice_rf_impute(df, max_iter=10, n_estimators=100, random_state=0):\n",
    "#     imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(n_estimators=n_estimators, random_state=random_state),\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=random_state\n",
    "#     )\n",
    "#     imputed_data = imputer.fit_transform(df)\n",
    "#     return pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# # 4. Deep Learning Imputation\n",
    "# def create_autoencoder(input_dim, encoding_dim):\n",
    "#     input_layer = Input(shape=(input_dim,))\n",
    "#     encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "#     decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "#     autoencoder = Model(input_layer, decoded)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     return autoencoder\n",
    "\n",
    "# def deep_learning_impute(df, epochs=100, batch_size=32):\n",
    "#     # Normalize data to [0, 1] range (suitable for beta values)\n",
    "#     normalized_data = df.values\n",
    "    \n",
    "#     # Create a mask for missing values\n",
    "#     missing_mask = np.isnan(normalized_data)\n",
    "    \n",
    "#     # Replace NaNs with mean for initial input\n",
    "#     col_mean = np.nanmean(normalized_data, axis=0)\n",
    "#     normalized_data[missing_mask] = np.take(col_mean, missing_mask.nonzero()[1])\n",
    "    \n",
    "#     # Create and train the autoencoder\n",
    "#     input_dim = df.shape[1]\n",
    "#     encoding_dim = min(input_dim // 2, 256)  # Cap encoding dim to prevent overfitting\n",
    "#     autoencoder = create_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "#     autoencoder.fit(normalized_data, normalized_data, \n",
    "#                     epochs=epochs, \n",
    "#                     batch_size=batch_size, \n",
    "#                     shuffle=True,\n",
    "#                     validation_split=0.2,\n",
    "#                     verbose=0)\n",
    "    \n",
    "#     # Use the model to impute missing values\n",
    "#     imputed_data = autoencoder.predict(normalized_data)\n",
    "    \n",
    "#     # Replace only the missing values in the original data\n",
    "#     normalized_data[missing_mask] = imputed_data[missing_mask]\n",
    "    \n",
    "#     return pd.DataFrame(normalized_data, columns=df.columns, index=df.index)\n",
    "\n",
    "# # 5. Ensemble Method\n",
    "# def ensemble_impute(df, methods=['knn', 'mice_rf', 'deep_learning']):\n",
    "#     imputed_dfs = []\n",
    "    \n",
    "#     if 'knn' in methods:\n",
    "#         imputed_dfs.append(knn_impute(df))\n",
    "#     if 'mice_rf' in methods:\n",
    "#         imputed_dfs.append(mice_rf_impute(df))\n",
    "#     if 'deep_learning' in methods:\n",
    "#         imputed_dfs.append(deep_learning_impute(df))\n",
    "    \n",
    "#     # Average the results from different methods\n",
    "#     ensemble_imputed = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "#     return ensemble_imputed\n",
    "\n",
    "# # 6. Validation and Sensitivity Analysis\n",
    "# def validate_imputation(original_df, imputed_df, n_samples=1000):\n",
    "#     # Select a subset of non-missing values to compare\n",
    "#     non_missing_mask = ~original_df.isnull().any(axis=1)\n",
    "#     sample_indices = np.random.choice(non_missing_mask.index[non_missing_mask], size=n_samples, replace=False)\n",
    "    \n",
    "#     original_sample = original_df.loc[sample_indices]\n",
    "#     imputed_sample = imputed_df.loc[sample_indices]\n",
    "    \n",
    "#     mse = mean_squared_error(original_sample.values.flatten(), imputed_sample.values.flatten())\n",
    "#     correlation, _ = pearsonr(original_sample.values.flatten(), imputed_sample.values.flatten())\n",
    "    for df in [methylation_data_final_pd_prepared, methylation_data_subset_pd_prepared]:\n",
    "    df['female'] = df['Female']\n",
    "    df['age'] = df['Age']\n",
    "#     print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "#     print(f\"Pearson Correlation: {correlation:.4f}\")\n",
    "    \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.scatter(original_sample.values.flatten(), imputed_sample.values.flatten(), alpha=0.1)\n",
    "#     plt.xlabel(\"Original Values\")\n",
    "#     plt.ylabel(\"Imputed Values\")\n",
    "#     plt.title(\"Original vs Imputed Values\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main workflow\n",
    "# def imputation_workflow(df, missing_features):\n",
    "#     methylation_data, metadata = prepare_data(df, missing_features)\n",
    "    \n",
    "#     print(\"Exploratory Analysis:\")\n",
    "#     exploratory_analysis(methylation_data)\n",
    "    \n",
    "#     print(\"\\nPerforming KNN Imputation...\")\n",
    "#     knn_imputed = knn_impute(methylation_data)\n",
    "    \n",
    "#     print(\"\\nPerforming MICE with Random Forest Imputation...\")\n",
    "#     mice_rf_imputed = mice_rf_impute(methylation_data)\n",
    "    \n",
    "#     print(\"\\nPerforming Deep Learning Imputation...\")\n",
    "#     dl_imputed = deep_learning_impute(methylation_data)\n",
    "    \n",
    "#     print(\"\\nPerforming Ensemble Imputation...\")\n",
    "#     ensemble_imputed = ensemble_impute(methylation_data)\n",
    "    \n",
    "#     print(\"\\nValidation and Sensitivity Analysis:\")\n",
    "#     validate_imputation(methylation_data, ensemble_imputed)\n",
    "    \n",
    "#     # Combine imputed data with metadata\n",
    "#     final_imputed_data = pd.concat([ensemble_imputed, metadata], axis=1)\n",
    "    \n",
    "#     return final_imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed_methylation_data = imputation_workflow(methylation_data_final_pd_prepared, grimage2_missing_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
